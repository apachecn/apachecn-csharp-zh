# GRU 与 LSTM、RNN 和前馈网络的比较

在本章中，我们将讨论**门控循环单元**（**GRU**）。我们还将把它们与我们在上一章中了解的 LSTM 进行比较。如你所知，LSTM 从 1987 年开始出现，是当今 NLP 深度学习中使用最广泛的模型之一。然而，GRU 首次出现于 2014 年，是 LSTM 的一个更简单的变体，具有许多相同的特性，训练更容易、更快，并且通常计算复杂度更低。

在本章中，我们将了解以下内容：

*   格鲁
*   GRU 与 LSTM 的区别
*   如何实现 GRU
*   GRU、LTSM、RNN 和前馈比较
*   网络差异

# 技术要求

您需要具备使用 Microsoft Visual Studio 和 C#进行.NET 开发的基本知识。您需要从图书网站下载本章的代码。

查看以下视频以查看代码的作用：[http://bit.ly/2OHd7o5](http://bit.ly/2OHd7o5) 。

# QuickNN

要遵循代码，您应该在 Microsoft Visual Studio 中打开 QuickNN 解决方案。我们将使用此代码详细解释一些更精细的点，以及不同网络编码之间的比较。以下是您应该加载的解决方案：

![](assets/4b001255-96d0-4503-b3a4-d79cf7be7ca0.png)

Solution

# 理解 GRU

GRU 是长短记忆递归神经网络的近亲。LSTM 和 GRU 网络都有额外的参数，用于控制内部内存的更新时间和方式。两者都可以捕获序列中的长期和短期依赖关系。然而，GRU 网络比 LSTM 网络涉及的参数更少，因此训练速度更快。GRU 学习如何使用重置和遗忘门，以便在执行内存保护的同时进行长期预测。让我们看一个 GRU 的简单图表：

![](assets/2efeafa7-af09-4338-bf4c-bba5286c6e1f.png)

GRU

# LSTM 和 GRU 之间的差异

LSTM 和 GRU 之间有一些细微的区别，尽管说实话，它们的相似之处多于不同之处！首先，GRU 比 LSTM 少一个门。如下图所示，LSTM 有一个输入门、一个忘记门和一个输出门。另一方面，GRU 只有两个门，一个复位门和一个更新门。重置门确定如何将新输入与以前的内存组合，更新门定义以前的内存的剩余量：

![](assets/2cd83c9c-b843-4099-9df8-98272cf6d59c.png)

LSTM vs GRU

另一个有趣的事实是，如果我们将重置门设置为所有 1，将更新门设置为所有 0，您知道我们有什么吗？如果你猜到了一个简单的旧的递归神经网络，你是对的！

以下是 LSTM 和 GRU 之间的主要区别：

*   GRU 有两个门，LSTM 有三个门。
*   GRU 没有与暴露隐藏状态不同的内部存储单元。这是因为 LSTM 的输出门不工作。
*   输入门和遗忘门由一个更新门耦合，更新门权衡新旧内容。
*   重置门直接应用于先前的隐藏状态。
*   在计算 GRU 输出时，我们不应用第二非线性。
*   没有输出门，加权和就是输出。

# 使用 GRU 与 LSTM

由于 GRU 相对较新，通常会出现使用哪种网络类型以及何时使用的问题。老实说，这里确实没有明确的赢家，因为 GRU 尚未成熟，而且 LSTM 变体似乎每个月都会出现。GRU 的参数确实较少，理论上可能比 LSTM 训练快一点。理论上，它可能比 LSTM 需要更少的数据。另一方面，如果您有大量数据，那么 LSTM 的额外功能可能会对您更好。

# 编码不同的网络

在本节中，我们将查看本章前面描述的示例代码。我们将特别关注如何构建不同的网络。`NetworkBuilder`是我们构建本次演习所需的四种不同类型网络的主要目标。如果您愿意，您可以随意修改它并添加其他网络。目前，它支持以下网络：

*   LSTM
*   循环神经网络
*   格鲁
*   前馈

在我们的示例网络中，您会注意到的一点是，网络之间的唯一区别在于网络本身是如何通过`NetworkBuilder`创建的。所有剩余的代码保持不变。如果查看示例源代码，您还会注意到 GRU 示例中的迭代次数或纪元次数要少得多。这是因为 GRU 通常更容易训练，因此需要更少的迭代。虽然我们的正常 RNN 训练大约在 50000 次迭代（以防万一，我们让它达到 100000 次）左右完成，但我们的 GRU 训练循环通常在 10000 次迭代以下完成，这是一个非常大的计算节省。

# 对 LSTM 进行编码

要构造 LSTM，我们只需调用我们的`NetworkBuilder`的`MakeLstm()`函数。此函数将接收多个输入参数并向我们返回一个网络对象：

```
INetwork nn = NetworkBuilder.MakeLstm(inputDimension,
hiddenDimension,hiddenLayers,outputDimension,data.GetModelOutputUnitToUse(),
initParamsStdDev, rng);
```

如您所见，它在内部调用`NetworkBuilder`对象中的`MakeLSTM()`函数。下面是该代码的一个示例：

```
public static NeuralNetwork MakeLstm(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)
{
List<ILayer> layers = new List<ILayer>();
for (int h = 0; h<hiddenLayers; h++)
{
```

添加所有隐藏层：

```
layers.Add(h == 0? new LstmLayer(inputDimension, hiddenDimension, initParamsStdDev, rng): new LstmLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));
}
```

添加前馈层：

```
layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));
```

创建网络：

```
return new NeuralNetwork(layers);
}
```

# 对 GRU 进行编码

为了构造一个选通循环单元，我们只需调用我们的`NetworkBuilder`的`MakeGru()`函数，如下所示：

```
INetwork nn = NetworkBuilder.MakeGru(inputDimension,
hiddenDimension,
hiddenLayers,
outputDimension,
data.GetModelOutputUnitToUse(),
initParamsStdDev, rng);
```

`MakeGru()`函数在内部调用相同的命名函数来构建我们的 GRU 网络。以下是它是如何做到这一点的：

```
public static NeuralNetwork MakeGru(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)
{
List<ILayer> layers = new List<ILayer>();
for (int h = 0; h<hiddenLayers; h++)
 {
 layers.Add(h == 0
 ? newGruLayer(inputDimension, hiddenDimension, initParamsStdDev, rng)
 : newGruLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));
 }
layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));
return new NeuralNetwork(layers);
}
```

# 比较 LSTM、GRU、前馈和 RNN 操作

为了帮助您看到我们所处理的所有网络对象的创建和结果的差异，我创建了下面的示例代码。通过此示例，您可以看到我们这里提供的所有四种网络类型的训练时间的差异。如前所述，GRU 是最容易训练的，因此比其他网络完成得更快（迭代次数更少）。在执行代码时，您将看到 GRU 通常在 10000 次迭代中达到最佳错误率，而常规 RNN 和/或 LSTM 可能需要 50000 次或更多次迭代才能正确收敛。

下面是我们的示例代码：

```
static void Main(string[] args)
{
Console.WriteLine("Running GRU sample", Color.Yellow);
Console.ReadKey();
ExampleGRU.Run();
Console.ReadKey();
Console.WriteLine("Running LSTM sample", Color.Yellow);
Console.ReadKey();
ExampleLSTM.Run();
Console.ReadKey();
Console.WriteLine("Running RNN sample", Color.Yellow);
Console.ReadKey();
ExampleRNN.Run();
Console.ReadKey();
Console.WriteLine("Running Feed Forward sample", Color.Yellow);
Console.ReadKey();
ExampleFeedForward.Run();
Console.ReadKey();
}
```

下面是运行示例的输出：

![](assets/b9cbe876-a369-4124-8446-6fb6922bd871.png)

Output 1

![](assets/5aa428f9-17b8-4150-876f-db6080f6bc73.png)

Output 2

现在，让我们看看如何创建 GRU 网络并运行程序。在下面的代码段中，我们将使用 XOR 数据集生成器为我们生成随机数据。对于我们的网络，我们将有 2 个输入，1 个带有 3 个神经元的隐藏层和 1 个输出。我们的学习率设置为 0.001，标准偏差设置为 0.08。

我们称我们的`NetworkBuilder`*对象，负责创建我们所有的网络变体。我们将所有描述的参数传递给`NetworkBuilder`。创建网络对象后，我们将此变量传递给培训师，并对网络进行培训。一旦网络培训完成，我们将测试我们的网络，以确保我们的结果令人满意。当我们创建用于测试的图形对象时，一定要将 false 传递给构造函数，让它知道我们不需要反向传播：*

 *```
public class ExampleGRU
 {
public static void Run()
 {
Random rng = new Random();
DataSet data = new XorDataSetGenerator();
int inputDimension = 2;
int hiddenDimension = 3;
int outputDimension = 1;
int hiddenLayers = 1;
double learningRate = 0.001;
double initParamsStdDev = 0.08;

INetwork nn = NetworkBuilder.MakeGru(inputDimension,
hiddenDimension, hiddenLayers, outputDimension, newSigmoidUnit(),
initParamsStdDev, rng);

int reportEveryNthEpoch = 10;
int trainingEpochs = 10000; // GRU's typically need less training
Trainer.train<NeuralNetwork>(trainingEpochs, learningRate, nn, data, reportEveryNthEpoch, rng);
Console.WriteLine("Training Completed.", Color.Green);
Console.WriteLine("Test: 1,1", Color.Yellow);
Matrix input = new Matrix(new double[] { 1, 1 });
Matrix output = nn.Activate(input, new Graph(false));
Console.WriteLine("Test: 1,1\. Output:" + output.W[0], Color.Yellow);
Matrix input1 = new Matrix(new double[] { 0, 1 });
Matrix output1 = nn.Activate(input1, new Graph(false));
Console.WriteLine("Test: 0,1\. Output:" + output1.W[0], Color.Yellow);
Console.WriteLine("Complete", Color.Yellow);
 }
 }
```

# 网络差异

如前所述，我们网络之间的唯一区别是创建并添加到网络对象的层。在 LSTM 中，我们将添加 LSTM 层，在 GRU 中，毫不奇怪，我们将添加 GRU 层，等等。所有四种类型的创建函数显示如下，供您比较：

```
public static NeuralNetwork MakeLstm(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)
{
    List<ILayer> layers = new List<ILayer>();
    for (int h = 0; h<hiddenLayers; h++)
    {
        layers.Add(h == 0
         ? new LstmLayer(inputDimension, hiddenDimension, initParamsStdDev, rng)
         : new LstmLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));
    }
    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit,         initParamsStdDev, rng));
    return new NeuralNetwork(layers);
}

public static NeuralNetwork MakeFeedForward(int inputDimension, int hiddenDimension, inthiddenLayers, int outputDimension, INonlinearity hiddenUnit, INonlinearity decoderUnit, double initParamsStdDev, Random rng)
{
    List<ILayer> layers = new List<ILayer>();
    for (int h = 0; h<hiddenLayers; h++)
    {
        layers.Add(h == 0? new FeedForwardLayer(inputDimension, hiddenDimension,         hiddenUnit, initParamsStdDev, rng): new FeedForwardLayer(hiddenDimension,         hiddenDimension, hiddenUnit, initParamsStdDev, rng));
    }
    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension,             decoderUnit, initParamsStdDev, rng));
    return new NeuralNetwork(layers);
 }

public static NeuralNetwork MakeGru(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity decoderUnit, double initParamsStdDev, Random rng)
{
    List<ILayer> layers = new List<ILayer>();
    for (int h = 0; h<hiddenLayers; h++)
    {
        layers.Add(h == 0? new GruLayer(inputDimension, hiddenDimension, initParamsStdDev, rng): new GruLayer(hiddenDimension, hiddenDimension, initParamsStdDev, rng));
    }
    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));
    return new NeuralNetwork(layers);
}

public static NeuralNetwork MakeRnn(int inputDimension, int hiddenDimension, int hiddenLayers, int outputDimension, INonlinearity hiddenUnit, INonlinearity decoderUnit, double initParamsStdDev, Random rng)
{
    List<ILayer> layers = new List<ILayer>();
    for (int h = 0; h<hiddenLayers; h++)
    {
        layers.Add(h == 0? new RnnLayer(inputDimension, hiddenDimension, hiddenUnit, initParamsStdDev, rng)
        : new RnnLayer(hiddenDimension, hiddenDimension, hiddenUnit, initParamsStdDev, rng));
    }
    layers.Add(new FeedForwardLayer(hiddenDimension, outputDimension, decoderUnit, initParamsStdDev, rng));
    return new NeuralNetwork(layers);
}
```

# 总结

在本章中，我们学习了 GRUs。我们展示了它们与 LSTM 网络的比较和区别。我们还向您展示了一个示例程序，该程序测试了我们讨论的所有网络类型，并生成了它们的输出。我们还比较了这些网络是如何创建的。

我希望你在这本书中与我一起度过了愉快的旅程。当我们作为作者试图更好地理解读者希望看到和听到的内容时，我欢迎您的建设性评论和反馈，这将有助于使本书和源代码更好。直到下一本书，快乐编码！*